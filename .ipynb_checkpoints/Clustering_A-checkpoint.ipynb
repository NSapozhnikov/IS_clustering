{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52cd6796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting SNP list data from file...\n",
      "The SNP list for 22 chromosome is: \n",
      "['rs2096537' 'rs2190742' 'rs9605145' ... 'rs6009945' 'rs9616812'\n",
      " 'rs9616816']\n",
      "With length of: 1149\n",
      "Extracting matrix data from file...\n",
      "Reshaping an array...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1348803/1913769608.py:71: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='./cachedir'\", use \"location='./cachedir'\" instead.\n",
      "  mem = Memory(cachedir='./cachedir')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m### HDSBCAN estimator\u001b[39;00m\n\u001b[1;32m     71\u001b[0m mem \u001b[38;5;241m=\u001b[39m Memory(cachedir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cachedir\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m clusterer \u001b[38;5;241m=\u001b[39m \u001b[43mhdbscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHDBSCAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcluster_selection_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecomputed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcluster_selection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleaf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcore_dist_n_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiss_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m### Calculate scores that measure the quality of the clustering\u001b[39;00m\n\u001b[1;32m     80\u001b[0m labels \u001b[38;5;241m=\u001b[39m clusterer\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/hdbscan/hdbscan_.py:919\u001b[0m, in \u001b[0;36mHDBSCAN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    911\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    912\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_kwargs)\n\u001b[1;32m    914\u001b[0m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_,\n\u001b[1;32m    915\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobabilities_,\n\u001b[1;32m    916\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_persistence_,\n\u001b[1;32m    917\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree,\n\u001b[1;32m    918\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_linkage_tree,\n\u001b[0;32m--> 919\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_spanning_tree) \u001b[38;5;241m=\u001b[39m \u001b[43mhdbscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_data:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prediction_data()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/hdbscan/hdbscan_.py:632\u001b[0m, in \u001b[0;36mhdbscan\u001b[0;34m(X, min_cluster_size, min_samples, alpha, cluster_selection_epsilon, metric, p, leaf_size, algorithm, memory, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, cluster_selection_method, allow_single_cluster, match_reference_implementation, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m                 (single_linkage_tree, result_min_span_tree) \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39mcache(\n\u001b[1;32m    626\u001b[0m                     _hdbscan_boruvka_balltree)(X, min_samples, alpha,\n\u001b[1;32m    627\u001b[0m                                                metric, p, leaf_size,\n\u001b[1;32m    628\u001b[0m                                                approx_min_span_tree,\n\u001b[1;32m    629\u001b[0m                                                gen_min_span_tree,\n\u001b[1;32m    630\u001b[0m                                                core_dist_n_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tree_to_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m                           \u001b[49m\u001b[43msingle_linkage_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcluster_selection_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mallow_single_cluster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmatch_reference_implementation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m\t\t\t\t\t\t   \u001b[49m\u001b[43mcluster_selection_epsilon\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    639\u001b[0m             (result_min_span_tree,)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/hdbscan/hdbscan_.py:56\u001b[0m, in \u001b[0;36m_tree_to_labels\u001b[0;34m(X, single_linkage_tree, min_cluster_size, cluster_selection_method, allow_single_cluster, match_reference_implementation, cluster_selection_epsilon)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tree_to_labels\u001b[39m(X, single_linkage_tree, min_cluster_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     49\u001b[0m                     cluster_selection_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meom\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     50\u001b[0m                     allow_single_cluster\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m                     match_reference_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m \t\t\t\t\tcluster_selection_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124;03m\"\"\"Converts a pretrained tree and cluster size into a\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    set of labels and probabilities.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     condensed_tree \u001b[38;5;241m=\u001b[39m \u001b[43mcondense_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_linkage_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     stability_dict \u001b[38;5;241m=\u001b[39m compute_stability(condensed_tree)\n\u001b[1;32m     59\u001b[0m     labels, probabilities, stabilities \u001b[38;5;241m=\u001b[39m get_clusters(condensed_tree,\n\u001b[1;32m     60\u001b[0m                                                       stability_dict,\n\u001b[1;32m     61\u001b[0m                                                       cluster_selection_method,\n\u001b[1;32m     62\u001b[0m                                                       allow_single_cluster,\n\u001b[1;32m     63\u001b[0m                                                       match_reference_implementation,\n\u001b[1;32m     64\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t\t  cluster_selection_epsilon)\n",
      "File \u001b[0;32mhdbscan/_hdbscan_tree.pyx:43\u001b[0m, in \u001b[0;36mhdbscan._hdbscan_tree.condense_tree\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mhdbscan/_hdbscan_tree.pyx:94\u001b[0m, in \u001b[0;36mhdbscan._hdbscan_tree.condense_tree\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Clusterization of LD matrices with dbscan and hdsbcan\n",
    "# Author: Nikita Sapozhnikov, info@inzilico.com\n",
    "# Date: May 29, 2023\n",
    "# best_params_clustering v0.2\n",
    "# Please check README.txt\n",
    "# python3 best_params_clustering.py /mnt/wd/nsap/in_data1/chr22_matrix.ld /mnt/wd/nsap/in_data2/chr22.snplist all\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.cluster import  DBSCAN \n",
    "from sklearn.metrics.cluster import *\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse    \n",
    "from getpass import getpass\n",
    "import re\n",
    "import configparser\n",
    "from joblib import Memory\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "main_fp = '/mnt/wd/nsap/'\n",
    "\n",
    "### Define the parameter grid\n",
    "params = {'eps': [0.1, 0.9],\n",
    "          'min_samples': [2, 4]}\n",
    "\n",
    "for num_chr in range(22,23):\n",
    "    ### open in-files with snplist and matrix for a chromosome\n",
    "    snp_file_path = f\"{main_fp}in_data1/chr{num_chr}.snplist\"\n",
    "    matrix_file_path = f\"{main_fp}in_data1/chr{num_chr}_matrix.ld\"\n",
    "    try:\n",
    "        with open(snp_file_path, 'r') as snp_file:\n",
    "            print('\\nExtracting SNP list data from file...')\n",
    "            snp_list = np.loadtxt(snp_file, dtype=str)\n",
    "            print(f\"The SNP list for {str(num_chr)} chromosome is: \\n{snp_list}\")\n",
    "            print(f\"With length of: {len(snp_list)}\")\n",
    "    except FileNotFoundError:\n",
    "        sys.exit('No such file.')      \n",
    "    try:\n",
    "        with open(matrix_file_path, 'r') as corr_file:\n",
    "            print('Extracting matrix data from file...')\n",
    "            corr_matrix = np.fromfile(corr_file, sep=' ')\n",
    "    except FileNotFoundError:\n",
    "        sys.exit('No such file.')\n",
    "    ### transforming into a dissimilarity matrix    \n",
    "    np.nan_to_num(corr_matrix, copy=False)  \n",
    "    print('Reshaping an array...')\n",
    "    diss_matrix = 1 - np.abs(corr_matrix, out=corr_matrix)\n",
    "    diss_matrix = diss_matrix.reshape(len(snp_list), len(snp_list))\n",
    "    np.fill_diagonal(diss_matrix, 0)\n",
    "    \n",
    "    ### Initialize an empty list to store the data\n",
    "    data = []\n",
    "    ### Iterate over the parameter grid\n",
    "    for eps in params['eps']:\n",
    "        for min_samples in params['min_samples']:\n",
    "            ### DBSCAN estimator\n",
    "            dbscan = DBSCAN(eps=eps, \n",
    "                            min_samples=min_samples, \n",
    "                            metric='precomputed', \n",
    "                            n_jobs=10).fit(diss_matrix) \n",
    "            ### Calculate scores that measure the quality of the clustering\n",
    "            labels = dbscan.labels_\n",
    "            dbscan_sil_score = silhouette_score(diss_matrix, labels) if len(set(labels)) > 1 else 0\n",
    "            dbscan_CH_score = calinski_harabasz_score(diss_matrix, labels) if len(set(labels)) > 1 else 0\n",
    "            ### append scores to list\n",
    "            data.append(['dbscan', eps, min_samples, dbscan_sil_score, dbscan_CH_score])\n",
    "            ### HDSBCAN estimator\n",
    "            mem = Memory(cachedir='./cachedir')\n",
    "            clusterer = hdbscan.HDBSCAN(min_cluster_size=8, \n",
    "                                        min_samples=min_samples,\n",
    "                                        cluster_selection_epsilon=eps,\n",
    "                                        metric='precomputed', \n",
    "                                        cluster_selection_method='leaf',\n",
    "                                        core_dist_n_jobs=6, \n",
    "                                        memory=mem).fit(diss_matrix)\n",
    "            ### Calculate scores that measure the quality of the clustering\n",
    "            labels = clusterer.labels_\n",
    "            hdbscan_sil_score = silhouette_score(diss_matrix, labels) if len(set(labels)) > 1 else 0\n",
    "            hdbscan_CH_score = calinski_harabasz_score(diss_matrix, labels) if len(set(labels)) > 1 else 0\n",
    "            mem.clear(warn=False)\n",
    "            ### append scores to list\n",
    "            data.append(['hdbscan', eps, min_samples, hdbscan_sil_score, hdbscan_CH_score])\n",
    "    pd_df = pd.DataFrame(data, columns=['algorithm', 'eps', 'min_samples', 'sil_score', 'CH_score'])\n",
    "print(pd_df)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0059d809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      True\n",
      "1     False\n",
      "2      True\n",
      "3     False\n",
      "4      True\n",
      "5     False\n",
      "6      True\n",
      "7     False\n",
      "8      True\n",
      "9     False\n",
      "10     True\n",
      "11    False\n",
      "12     True\n",
      "13    False\n",
      "14     True\n",
      "15    False\n",
      "16     True\n",
      "17    False\n",
      "Name: algorithm, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Define the iterators for 'eps' and 'min_samples'\n",
    "eps_values = [0.1, 0.2, 0.3]\n",
    "min_samples_values = [2, 5, 10]\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through the values and calculate 'sil_score' and 'CH_score'\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        sil_score = eps-min_samples\n",
    "        CH_score = eps+min_samples\n",
    "        data.append(['dbscan', eps, min_samples, sil_score, CH_score])\n",
    "        data.append(['hdbscan', eps, min_samples, sil_score, CH_score])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=['algorithm', 'eps', 'min_samples', 'sil_score', 'CH_score'])\n",
    "print(df['algorithm']=='dbscan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cc7ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=pd.DataFrame(columns=['algorithm', 'eps', 'min_samples', 'sil_score', 'CH_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0378160d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  algorithm  eps  min_samples  sil_score  CH_score\n",
      "0    dbscan    1            2        0.5       0.5\n"
     ]
    }
   ],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea126e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.loc[len(columns.index)] = ['dbscan',1,2,0.5,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "974ff938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {'eps': a,\n",
    "          'min_samples': b = np.arange(2,20,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3ae8629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648\n"
     ]
    }
   ],
   "source": [
    "b = np.arange(2,20,1)\n",
    "print(18*18*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e79fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
